# ==============================================================================
# src/predict_user_comments.py
# FÄ°NAL: Kendi YouTube yorumlarÄ±mÄ±zÄ± tahmin etme modÃ¼lÃ¼.
# ==============================================================================

import pandas as pd
import numpy as np
import joblib
import os
import re
import nltk
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# ------------------------------------------------------------------------------
# A. CONFIGURATION (YAPILANDIRMA)
# ------------------------------------------------------------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)

# Gerekli Dosyalar
USER_DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'user_comments_metadata.csv')
WORD2VEC_MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'word2vec_model.bin')
SCALER_PATH = os.path.join(PROJECT_ROOT, 'models', 'scaler.joblib')

# EN Ä°YÄ° MODELÄ°MÄ°ZÄ°N ADI (KlasÃ¶rdekiyle birebir aynÄ± olmalÄ±)
MLP_MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'Model_1_Genis_ve_Kontrollu.joblib')

# Ã‡Ä±ktÄ± DosyasÄ±
OUTPUT_PATH = os.path.join(PROJECT_ROOT, 'data', 'user_comments_predicted.csv')

# Word2Vec AyarÄ± (EÄŸitimdekiyle AYNI olmalÄ±)
VECTOR_SIZE = 300 

# ------------------------------------------------------------------------------
# B. PREPROCESSING (EÄŸitimdeki "Duygu Analizi Ã–zel" Versiyonuyla AYNI)
# ------------------------------------------------------------------------------
# NLTK KontrolÃ¼
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

def clean_and_tokenize(text):
    """Metni temizler ve token'lara ayÄ±rÄ±r. STOPWORDS SÄ°LÄ°NMEZ!"""
    if not isinstance(text, str):
        return []
    
    # 1. Temizlik
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
    text = re.sub(r'[^\w\s]', ' ', text) # NoktalamalarÄ± boÅŸluÄŸa Ã§evir
    text = re.sub(r'\d+', '', text)   
    
    # 2. Tokenize
    try:
        tokens = word_tokenize(text, language='turkish')
    except LookupError:
        tokens = word_tokenize(text, language='turkish')
        
    # 3. Filtreleme (Sadece tek harflileri atÄ±yoruz, "ama", "deÄŸil" kalÄ±yor!)
    tokens = [word for word in tokens if len(word) > 1]
    
    return tokens

def get_sentence_vector(text_tokens, model):
    vec = np.zeros(VECTOR_SIZE)
    count = 0
    for word in text_tokens:
        if word in model.wv:
            vec += model.wv[word]
            count += 1
    if count != 0:
        vec /= count
    return vec

# ------------------------------------------------------------------------------
# C. MAIN EXECUTION (ANA Ã‡ALIÅžTIRMA)
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    print("â³ Tahmin Ä°ÅŸlemi BaÅŸlatÄ±lÄ±yor...")
    
    # 1. Dosya Kontrolleri
    if not os.path.exists(USER_DATA_PATH):
        print(f"âŒ Hata: Yorum dosyasÄ± yok: {USER_DATA_PATH}")
        print("LÃ¼tfen Ã¶nce data_acquisition.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±p veri Ã§ekin.")
        exit()
        
    if not os.path.exists(MLP_MODEL_PATH):
        print(f"âŒ Hata: Model dosyasÄ± yok: {MLP_MODEL_PATH}")
        print("LÃ¼tfen Ã¶nce mlp_classifier.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.")
        exit()

    # 2. Modelleri YÃ¼kle
    print("ðŸ“¥ Modeller yÃ¼kleniyor (Word2Vec, Scaler, MLP)...")
    try:
        w2v_model = Word2Vec.load(WORD2VEC_MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        mlp_model = joblib.load(MLP_MODEL_PATH)
    except Exception as e:
        print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        exit()
    
    # 3. Veriyi YÃ¼kle
    df = pd.read_csv(USER_DATA_PATH)
    print(f"âœ… {len(df)} adet yorum yÃ¼klendi.")
    
    # SÃ¼tun adÄ± bulma (Bizim kodumuz 'Yorum' olarak kaydediyor ama garantilemek iÃ§in)
    col_name = None
    possible_names = ['Yorum', 'Yorum_Metni', 'Comment_Text']
    for name in possible_names:
        if name in df.columns:
            col_name = name
            break
            
    if col_name is None:
        print(f"âŒ Hata: Yorum sÃ¼tunu bulunamadÄ±. Mevcut sÃ¼tunlar: {list(df.columns)}")
        exit()
        
    # 4. Ã–zellik Ã‡Ä±karÄ±mÄ± (Vectorization)
    print("âš™ï¸  Yorumlar vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    df['tokens'] = df[col_name].apply(clean_and_tokenize)
    
    features = []
    for tokens in df['tokens']:
        vec = get_sentence_vector(tokens, w2v_model)
        features.append(vec)
    X_user = np.array(features)
    
    # 5. Scaling (Normalizasyon - Ã‡OK Ã–NEMLÄ°)
    # EÄŸitimde kullandÄ±ÄŸÄ±mÄ±z scaler ile aynÄ± dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapÄ±yoruz
    X_user = scaler.transform(X_user)
    
    # 6. Tahmin Yapma
    print("ðŸ”® Model tahmin yapÄ±yor...")
    predictions_encoded = mlp_model.predict(X_user)
    
    # 7. SonuÃ§larÄ± Etiketleme
    # Alfabetik SÄ±ra: 0=NÃ¶tr, 1=Olumlu, 2=Olumsuz
    label_map = {0: 'NÃ¶tr', 1: 'Olumlu', 2: 'Olumsuz'}
    
    df['Tahmin_Edilen_Duygu'] = [label_map.get(p, "Bilinmiyor") for p in predictions_encoded]
    
    # Sadece gerekli sÃ¼tunlarÄ± kaydet
    df_result = df[[col_name, 'Tahmin_Edilen_Duygu']]
    df_result.to_csv(OUTPUT_PATH, index=False)
    
    print("\n" + "="*60)
    print("ðŸ“‹ Ã–RNEK TAHMÄ°NLER (Ä°lk 15 Yorum)")
    print("="*60)
    pd.set_option('display.max_colwidth', 80) # YorumlarÄ±n tamamÄ±nÄ± gÃ¶relim
    print(df_result.head(15).to_string(index=False))
    print("\n" + "="*60)
    print(f"âœ… BÄ°TTÄ°! TÃ¼m tahminler ÅŸuraya kaydedildi:\n   -> {OUTPUT_PATH}")